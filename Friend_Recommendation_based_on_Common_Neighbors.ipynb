{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Friend Recommendation System using Common Neighbors"
      ],
      "metadata": {
        "id": "CqgXgux08LxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Description\n",
        "The data ([連結文字](https://snap.stanford.edu/data/ego-Facebook.html)) was collected from survey participants using a Facebook app developed by Stanford University. The data includes information from 4,031 users' friend networks, and the user IDs are encapsulated by some virtual user IDs."
      ],
      "metadata": {
        "id": "tPRp8Tix1-zX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Structure\n",
        "user1   friend_1<br>\n",
        "user1   friend_2<br>\n",
        "...<br>\n",
        "user4031 friend_k"
      ],
      "metadata": {
        "id": "CwCrx-e24KHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Friend Recommendation based on Common Friends\n",
        "Suggest friends to each user based on similarity, where the similarity is estimated by common friends."
      ],
      "metadata": {
        "id": "T5SsVsIQ6OTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Installization"
      ],
      "metadata": {
        "id": "nWTf1cJW6B4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td69WK3w18rV",
        "outputId": "06bb9160-94f0-488e-8df6-037ad5e5967b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ec2de98332350ab807d98a69f6cfb9ac5cb401876c6067e49d7db5391866adcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: findspark, pyspark\n",
            "Successfully installed findspark-2.0.1 pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_line(line): # read the node_id and friend_id's from a record\n",
        "  parts = line.split()\n",
        "  node_id = int(parts[0])\n",
        "  friend_id = int(parts[1])\n",
        "  return (node_id, friend_id)"
      ],
      "metadata": {
        "id": "Xk_i_sYk5h5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Steps"
      ],
      "metadata": {
        "id": "vGjJrth777Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FriendRecommendation:\n",
        "\n",
        "  def __init__(self, spark, file_path):\n",
        "    self.spark = spark\n",
        "    self.sc = spark.sparkContext # # instantiate a Spark Context\n",
        "    self.file_path = file_path\n",
        "\n",
        "  def run_recommendation(self, k = 10, output_file = False):\n",
        "\n",
        "    def parse_line(line): # read the node_id and friend_id's from a record\n",
        "      parts = line.split()\n",
        "      node_id = int(parts[0])\n",
        "      friend_id = int(parts[1])\n",
        "      return (node_id, friend_id)\n",
        "\n",
        "    matrix_rdd = self.sc.textFile(self.file_path) # read the text file\n",
        "    parsed_matrix_rdd = matrix_rdd.map(parse_line) # output: [(node_id, friend_id), ...]\n",
        "    # aggregate the friends from each node into a list\n",
        "    node_neighbors_rdd = parsed_matrix_rdd.groupByKey().mapValues(list) # [(node_id, list_of_frds), ...]\n",
        "    # generate a sequence of node pairs, where pairs of two same nodes are excluded (no interested in common friends of two same nodes)\n",
        "    node_id_pairs = node_neighbors_rdd.cartesian(node_neighbors_rdd).filter(lambda x: x[0][0] != x[1][0])\n",
        "\n",
        "    def find_common_neighbors(pair): # return number of common friends of a pair of nodes\n",
        "      list1, list2 = pair[0][1], pair[1][1]\n",
        "      neighbors1 = set(list1)\n",
        "      neighbors2 = set(list2)\n",
        "      return len(neighbors1.intersection(neighbors2))\n",
        "\n",
        "    # input: pair of nodes; output: [(node1, node2, #common_frds), ...] (sorted by node1 ASC and #common_frds DESC)\n",
        "    node_common_neighbors = node_id_pairs.map(lambda pair: (pair[0][0], pair[1][0], find_common_neighbors(pair))).sortBy(lambda x: (x[0], -1 * x[2]))\n",
        "\n",
        "    node_common_neighbors_new = node_common_neighbors.map(lambda pair: (pair[0], pair[1]))\n",
        "    node_common_neighbors_new = node_common_neighbors_new.groupByKey().mapValues(list)\n",
        "    # output: [(node_id, list_of_frds), ...] (sorted by list_of_frds DESC)\n",
        "\n",
        "    # exclude the existing frds from recommendation\n",
        "    my_dict = node_neighbors_rdd.collectAsMap()\n",
        "    def exclude_existing_frds(node_id, frd_list):\n",
        "      return (node_id, [ele for ele in frd_list if ele not in my_dict[node_id]])\n",
        "    result_rdd = node_common_neighbors_new.map(lambda x: exclude_existing_frds(x[0], x[1]))\n",
        "    new_rdd = result_rdd.mapValues(lambda x: x[:k]).map(lambda x: (x[0], x[1])).sortBy(lambda x: x[0])\n",
        "    # output: the k most recommended friends to each node: [(node_id, [frd1, ..., frd_k]), ...]\n",
        "\n",
        "    # output the recommendation result to a textfile\n",
        "    if output_file:\n",
        "      with open(\"Recommendation based on Common Neighbors.txt\", \"w\") as f:\n",
        "        for line in new_rdd.collect():\n",
        "          f.write(f\"{line}\\n\")\n",
        "\n",
        "    return new_rdd"
      ],
      "metadata": {
        "id": "dmj3ULGLBDnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "instantiate a SparkSession\n",
        "use all the available threads\n",
        "define the app name\n",
        "\"\"\"\n",
        "spark = SparkSession.builder \\\n",
        "     .master(\"local[*]\") \\\n",
        "     .appName(\"Friends' Recommendation\") \\\n",
        "     .getOrCreate()\n",
        "\n",
        "# create an instance of the class and run the recommendation\n",
        "recommendation = FriendRecommendation(spark, \"/content/sample_data/facebook_entire.txt\")\n",
        "result_rdd = recommendation.run_recommendation(k = 10, output_file = True) # recommend k friends to each node and generate the output to a textfile\n",
        "\n",
        "spark.stop() # terminate the SparkSession"
      ],
      "metadata": {
        "id": "7iM8Ew_RJ8Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output File\n",
        "The output file is available on [連結文字](https://drive.google.com/file/d/1OysBhMlYp7xDmboTYtWfFVmD4TofiZB-/view?usp=sharing)."
      ],
      "metadata": {
        "id": "gEa6tN32-n_n"
      }
    }
  ]
}